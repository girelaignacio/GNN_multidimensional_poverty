{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3QBFCJifrdZ0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric.datasets as datasets\n",
    "import torch_geometric.data\n",
    "import torch_geometric.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read spatial data: Nigeria DHS 20-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uR4QbIm94HSw"
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "df = gpd.read_file(\"../data/nga_dhs20-21.shp\")\n",
    "gdf = gpd.GeoDataFrame(df, crs=\"EPSG:4326\")\n",
    "#gdf = gdf.to_crs(\"EPSG:32617\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "BRVzB0Jp65lW",
    "outputId": "f473b51e-a908-4bed-ceb4-c146d6d85c82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GeometryArray>\n",
       "[ <POINT (8.507 7.72)>, <POINT (8.552 7.718)>, <POINT (8.991 7.347)>,\n",
       "   <POINT (8.15 7.21)>, <POINT (8.408 6.881)>, <POINT (7.951 7.805)>,\n",
       " <POINT (7.838 7.595)>, <POINT (8.125 7.605)>, <POINT (8.503 7.689)>,\n",
       " <POINT (8.582 7.845)>,\n",
       " ...\n",
       "  <POINT (3.19 8.318)>, <POINT (4.022 7.953)>, <POINT (4.191 8.202)>,\n",
       " <POINT (4.431 7.977)>,  <POINT (4.104 7.85)>, <POINT (3.588 7.828)>,\n",
       "  <POINT (3.343 8.05)>, <POINT (3.018 7.948)>, <POINT (3.291 7.341)>,\n",
       " <POINT (3.948 7.786)>]\n",
       "Length: 1383, dtype: geometry"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.geometry.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cTvhxUcW5qgp",
    "outputId": "990f876c-968f-4d49-9725-a3b13fd93b64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\girel\\AppData\\Local\\Temp\\ipykernel_32440\\4188193525.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroids = np.array([(point.x, point.y) for point in df.geometry.centroid])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.506936, 7.720049],\n",
       "       [8.552142, 7.717922],\n",
       "       [8.991185, 7.346602],\n",
       "       ...,\n",
       "       [3.017661, 7.947632],\n",
       "       [3.291019, 7.341286],\n",
       "       [3.948039, 7.785664]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids = np.array([(point.x, point.y) for point in df.geometry.centroid])\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build distance matrix\n",
    "\n",
    "We build a proximity matrix between the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "lx3-Bvbl5qsk",
    "outputId": "534c288e-adc0-4b3d-8eba-a18a3bba16e2"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "distance_matrix = cdist(np.unique(centroids, axis = 0), np.unique(centroids, axis = 0), 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0Rs-v7sM5q5T"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  7.90917123,  7.11335248, ..., 16.77679502,\n",
       "        17.73958348, 18.81761545],\n",
       "       [ 7.90917123,  0.        ,  0.8931784 , ..., 10.93767371,\n",
       "        11.43550126, 12.22038214],\n",
       "       [ 7.11335248,  0.8931784 ,  0.        , ..., 11.1203557 ,\n",
       "        11.72264204, 12.58043899],\n",
       "       ...,\n",
       "       [16.77679502, 10.93767371, 11.1203557 , ...,  0.        ,\n",
       "         1.47642768,  2.79842185],\n",
       "       [17.73958348, 11.43550126, 11.72264204, ...,  1.47642768,\n",
       "         0.        ,  1.33305874],\n",
       "       [18.81761545, 12.22038214, 12.58043899, ...,  2.79842185,\n",
       "         1.33305874,  0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.817615452436154"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_distance = np.max(distance_matrix)\n",
    "max_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_proximity_matrix = np.zeros_like(distance_matrix)\n",
    "for i in range(len(distance_matrix)):\n",
    "    for j in range(len(distance_matrix)):\n",
    "        proximity = (max_distance - distance_matrix[i, j]) / max_distance\n",
    "        proximity = max(0, proximity)\n",
    "        weighted_proximity_matrix[i, j] = proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(weighted_proximity_matrix.min(),weighted_proximity_matrix.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.99\n",
    "weighted_proximity_matrix[weighted_proximity_matrix < threshold] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_proximity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_proximity_matrix[np.diag_indices_from(weighted_proximity_matrix)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transform data to tensor\n",
    "\n",
    "Create torch_geometric.data.Data object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform proximity matrix to tensor and create edges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = torch.tensor(weighted_proximity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = adj_matrix.nonzero().t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    2,    2,  ..., 1380, 1380, 1380],\n",
       "        [   4,    5,    6,  ..., 1375, 1376, 1377]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get edge attributes (given by proximity of clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WIBVjlFAI4Vt"
   },
   "outputs": [],
   "source": [
    "edge_attr = []\n",
    "for i in range(edges.size(1)):\n",
    "    edge_attr.append([adj_matrix[edges[0][i], edges[1][0]]])\n",
    "\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dhsclst', 'd_cm', 'd_nutr', 'd_satt', 'd_educ', 'd_elct', 'd_wtr',\n",
       "       'd_sani', 'd_hsg', 'd_ckfl', 'd_asst', 'score', 'sexfeml', 'sexmale',\n",
       "       'agc70_4', 'a710_14', 'a715_17', 'a718_59', 'agc75_9', 'agc760_',\n",
       "       'arearrl', 'arearbn', 'reginAb', 'rgnAdmw', 'rgnAk_I', 'rgnAnmb',\n",
       "       'regnBch', 'rgnByls', 'reginBn', 'regnBrn', 'rgnCr_R', 'regnDlt',\n",
       "       'rgnEbny', 'reginEd', 'regnEkt', 'regnEng', 'regnFCT', 'regnGmb',\n",
       "       'reginIm', 'regnJgw', 'regnKdn', 'reginKn', 'rgnKtsn', 'regnKbb',\n",
       "       'reginKg', 'regnKwr', 'regnLgs', 'rgnNsrw', 'regnNgr', 'regnOgn',\n",
       "       'regnOnd', 'regnOsn', 'reginOy', 'regnPlt', 'rgnRvrs', 'regnSkt',\n",
       "       'regnTrb', 'reginYb', 'rgnZmfr', 'hdshpf_', 'hdshpm_', 'categry',\n",
       "       'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(gdf.columns[list(range(1, gdf.shape[1]-4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid multicollineality\n",
    "for i in ['sexmale','agc760_','arearrl','reginAb','score']:#,'hdshpm_']:#,\n",
    "         #'d_cm', 'd_nutr', 'd_satt', 'd_educ']:\n",
    "    features.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d_cm', 'd_nutr', 'd_satt', 'd_educ', 'd_elct', 'd_wtr', 'd_sani', 'd_hsg', 'd_ckfl', 'd_asst', 'sexfeml', 'agc70_4', 'a710_14', 'a715_17', 'a718_59', 'agc75_9', 'arearbn', 'rgnAdmw', 'rgnAk_I', 'rgnAnmb', 'regnBch', 'rgnByls', 'reginBn', 'regnBrn', 'rgnCr_R', 'regnDlt', 'rgnEbny', 'reginEd', 'regnEkt', 'regnEng', 'regnFCT', 'regnGmb', 'reginIm', 'regnJgw', 'regnKdn', 'reginKn', 'rgnKtsn', 'regnKbb', 'reginKg', 'regnKwr', 'regnLgs', 'rgnNsrw', 'regnNgr', 'regnOgn', 'regnOnd', 'regnOsn', 'reginOy', 'regnPlt', 'rgnRvrs', 'regnSkt', 'regnTrb', 'reginYb', 'rgnZmfr']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wTOdbO8IQI4f"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(np.array(gdf[features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"reg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0804, 0.0678, 0.0451,  ..., 0.1570, 0.5354, 0.1026])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if task == \"reg\":\n",
    "    target_tensor = torch.tensor(gdf['score'], dtype=torch.float)\n",
    "else: \n",
    "    target = pd.Categorical(gdf['categry'])\n",
    "    numerical_categories = target.codes\n",
    "    print(np.unique(numerical_categories))\n",
    "    target_tensor = torch.tensor(numerical_categories, dtype=torch.long) \n",
    "target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYrOagwsPxlj",
    "outputId": "703aa669-2e31-4795-c53f-fe6d45373a3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1389, 53], edge_index=[2, 8914], edge_attr=[8914, 1], y=[1389])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = torch_geometric.data.Data(x=x, edge_index=edges, edge_attr = edge_attr,y=target_tensor)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "c1BM5KgFQyxZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 46 artists>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import degree\n",
    "from collections import Counter\n",
    "\n",
    "# Get list of degrees for each node\n",
    "degrees = degree(graph.edge_index[0]).numpy()\n",
    "\n",
    "# Count the number of nodes for each degree\n",
    "numbers = Counter(degrees)\n",
    "\n",
    "# Bar plot\n",
    "fig, ax = plt.subplots(figsize=(18, 7))\n",
    "ax.set_xlabel('Node degree')\n",
    "ax.set_ylabel('Number of nodes')\n",
    "plt.bar(numbers.keys(),\n",
    "        numbers.values(),\n",
    "        color='#0A047A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1389"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of nodes\n",
    "num_nodes = graph.num_nodes\n",
    "num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111 138 140\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sizes of the train, validation, and test sets\n",
    "train_size = int(0.8 * num_nodes)\n",
    "val_size = int(0.1 * num_nodes)\n",
    "test_size = num_nodes - train_size - val_size\n",
    "print(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1389, 53], edge_index=[2, 8914], edge_attr=[8914, 1], y=[1389], train_mask=[1389], val_mask=[1389], test_mask=[1389])\n"
     ]
    }
   ],
   "source": [
    "# Create random permutations of node indices\n",
    "# Set the random seed\n",
    "seed = 91218\n",
    "torch.manual_seed(seed)\n",
    "perm = torch.randperm(num_nodes)\n",
    "\n",
    "# Create boolean masks\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[perm[:train_size]] = True\n",
    "val_mask[perm[train_size:train_size + val_size]] = True\n",
    "test_mask[perm[train_size + val_size:]] = True\n",
    "\n",
    "# Add the masks to the graph object\n",
    "graph.train_mask = train_mask\n",
    "graph.val_mask = val_mask\n",
    "graph.test_mask = test_mask\n",
    "\n",
    "print(graph) #now the graph contains the masks."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "train_loader = NeighborLoader(\n",
    "    graph,\n",
    "    input_nodes = graph.train_mask,\n",
    "    batch_size = int(train_size//8),\n",
    "    num_neighbors = [1,10],\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_loader = NeighborLoader(\n",
    "    graph,\n",
    "    input_nodes = graph.val_mask,\n",
    "    batch_size = int(val_size//3),\n",
    "    num_neighbors = [1,10]\n",
    ")\n",
    "\n",
    "test_loader = NeighborLoader(\n",
    "    graph,\n",
    "    input_nodes = graph.test_mask,\n",
    "    batch_size = int(test_size//3),\n",
    "    num_neighbors = [1,10]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #check if cuda is available\n",
    "graph = graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "train_nodes = torch.where(train_mask)[0]\n",
    "sub_edge_index, _ = subgraph(train_mask, graph.edge_index, relabel_nodes=True)\n",
    "train_x = graph.x[train_nodes]\n",
    "train_y = graph.y[train_nodes]\n",
    "train_data = torch_geometric.data.Data(x=train_x, y=train_y, edge_index=sub_edge_index)\n",
    "train_loader = torch_geometric.loader.DataLoader([train_data], batch_size=int(train_size//20), shuffle=True)\n",
    "\n",
    "val_nodes = torch.where(val_mask)[0]\n",
    "sub_edge_index, _ = subgraph(val_mask, graph.edge_index, relabel_nodes=True)\n",
    "val_x = graph.x[val_nodes]\n",
    "val_y = graph.y[val_nodes]\n",
    "val_data = torch_geometric.data.Data(x=val_x, y=val_y, edge_index=sub_edge_index)\n",
    "val_loader = torch_geometric.loader.DataLoader([val_data], batch_size=val_size)\n",
    "\n",
    "test_nodes = torch.where(test_mask)[0]\n",
    "sub_edge_index, _ = subgraph(test_mask, graph.edge_index, relabel_nodes=True)\n",
    "test_x = graph.x[test_nodes]\n",
    "test_y = graph.y[test_nodes]\n",
    "test_data = torch_geometric.data.Data(x=test_x, y=test_y, edge_index=sub_edge_index)\n",
    "test_loader = torch_geometric.loader.DataLoader([test_data], batch_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "cWFE3gnUo7nA"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Dropout\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv, GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention network\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_h, dim_out, n_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat1 = GATv2Conv(dim_in, dim_h, heads = n_heads)\n",
    "        self.norm1 = torch.nn.LayerNorm(dim_h * n_heads)\n",
    "        self.gat2 = GATv2Conv(dim_h*n_heads, dim_out, heads = 1)\n",
    "        self.norm2 = torch.nn.LayerNorm(16*n_heads)\n",
    "        self.gat3 = GATv2Conv(n_heads*n_heads, 16, heads = 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x1 = F.dropout(x, p = 0.7, training = self.training)\n",
    "        x = self.gat1(x1, edge_index)\n",
    "        x = self.norm1(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x, F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = graph.x.size(1)  \n",
    "dim_h = 32\n",
    "if task == \"reg\":\n",
    "    dim_out = 1\n",
    "else:\n",
    "    dim_out = len(graph.y.unique()) # Number of classes \n",
    "n_heads = 16\n",
    "\n",
    "# Create an instance of your GAT model\n",
    "model = GAT(dim_in, dim_h, dim_out, n_heads)\n",
    "untrained = GAT(dim_in, dim_h, dim_out, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(),weight_decay=5e-4)\n",
    "if task == \"reg\":\n",
    "    criterion = torch.nn.MSELoss()\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "n_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (gat1): GATv2Conv(53, 32, heads=16)\n",
      "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (gat2): GATv2Conv(512, 1, heads=1)\n",
      "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (gat3): GATv2Conv(256, 16, heads=1)\n",
      "  (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear1): Linear(in_features=16, out_features=25, bias=True)\n",
      "  (linear2): Linear(in_features=25, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Train Loss: 1.8337, Val Loss: 0.9438, Test Loss: 1.1203\n",
      "Epoch 2/500, Train Loss: 1.3458, Val Loss: 0.6637, Test Loss: 0.7999\n",
      "Epoch 3/500, Train Loss: 1.3302, Val Loss: 0.2949, Test Loss: 0.3338\n",
      "Epoch 4/500, Train Loss: 0.8121, Val Loss: 0.3862, Test Loss: 0.3620\n",
      "Epoch 5/500, Train Loss: 0.7629, Val Loss: 0.6040, Test Loss: 0.5705\n",
      "Epoch 6/500, Train Loss: 0.7982, Val Loss: 0.5152, Test Loss: 0.4919\n",
      "Epoch 7/500, Train Loss: 0.7179, Val Loss: 0.2531, Test Loss: 0.2348\n",
      "Epoch 8/500, Train Loss: 0.5702, Val Loss: 0.1175, Test Loss: 0.1005\n",
      "Epoch 9/500, Train Loss: 0.5091, Val Loss: 0.1347, Test Loss: 0.1190\n",
      "Epoch 10/500, Train Loss: 0.6930, Val Loss: 0.1717, Test Loss: 0.1561\n",
      "Epoch 11/500, Train Loss: 0.6000, Val Loss: 0.1607, Test Loss: 0.1460\n",
      "Epoch 12/500, Train Loss: 0.5651, Val Loss: 0.1302, Test Loss: 0.1179\n",
      "Epoch 13/500, Train Loss: 0.5601, Val Loss: 0.1216, Test Loss: 0.1124\n",
      "Epoch 14/500, Train Loss: 0.4967, Val Loss: 0.1341, Test Loss: 0.1280\n",
      "Epoch 15/500, Train Loss: 0.5518, Val Loss: 0.1418, Test Loss: 0.1370\n",
      "Epoch 16/500, Train Loss: 0.4477, Val Loss: 0.1334, Test Loss: 0.1298\n",
      "Epoch 17/500, Train Loss: 0.4653, Val Loss: 0.1045, Test Loss: 0.1014\n",
      "Epoch 18/500, Train Loss: 0.4871, Val Loss: 0.0667, Test Loss: 0.0661\n",
      "Epoch 19/500, Train Loss: 0.4129, Val Loss: 0.0445, Test Loss: 0.0486\n",
      "Epoch 20/500, Train Loss: 0.4292, Val Loss: 0.0493, Test Loss: 0.0585\n",
      "Epoch 21/500, Train Loss: 0.4587, Val Loss: 0.0581, Test Loss: 0.0712\n",
      "Epoch 22/500, Train Loss: 0.4903, Val Loss: 0.0540, Test Loss: 0.0670\n",
      "Epoch 23/500, Train Loss: 0.5048, Val Loss: 0.0443, Test Loss: 0.0529\n",
      "Epoch 24/500, Train Loss: 0.4181, Val Loss: 0.0567, Test Loss: 0.0607\n",
      "Epoch 25/500, Train Loss: 0.4735, Val Loss: 0.0856, Test Loss: 0.0868\n",
      "Epoch 26/500, Train Loss: 0.3930, Val Loss: 0.0939, Test Loss: 0.0947\n",
      "Epoch 27/500, Train Loss: 0.4125, Val Loss: 0.0746, Test Loss: 0.0768\n",
      "Epoch 28/500, Train Loss: 0.4158, Val Loss: 0.0389, Test Loss: 0.0437\n",
      "Epoch 29/500, Train Loss: 0.3584, Val Loss: 0.0194, Test Loss: 0.0263\n",
      "Epoch 30/500, Train Loss: 0.3717, Val Loss: 0.0140, Test Loss: 0.0215\n",
      "Epoch 31/500, Train Loss: 0.3779, Val Loss: 0.0132, Test Loss: 0.0205\n",
      "Epoch 32/500, Train Loss: 0.3686, Val Loss: 0.0138, Test Loss: 0.0202\n",
      "Epoch 33/500, Train Loss: 0.3916, Val Loss: 0.0187, Test Loss: 0.0241\n",
      "Epoch 34/500, Train Loss: 0.3689, Val Loss: 0.0287, Test Loss: 0.0332\n",
      "Epoch 35/500, Train Loss: 0.3699, Val Loss: 0.0456, Test Loss: 0.0498\n",
      "Epoch 36/500, Train Loss: 0.3918, Val Loss: 0.0547, Test Loss: 0.0587\n",
      "Epoch 37/500, Train Loss: 0.3388, Val Loss: 0.0512, Test Loss: 0.0544\n",
      "Epoch 38/500, Train Loss: 0.3472, Val Loss: 0.0378, Test Loss: 0.0401\n",
      "Epoch 39/500, Train Loss: 0.3490, Val Loss: 0.0223, Test Loss: 0.0243\n",
      "Epoch 40/500, Train Loss: 0.3270, Val Loss: 0.0145, Test Loss: 0.0170\n",
      "Epoch 41/500, Train Loss: 0.3490, Val Loss: 0.0136, Test Loss: 0.0165\n",
      "Epoch 42/500, Train Loss: 0.3474, Val Loss: 0.0141, Test Loss: 0.0172\n",
      "Epoch 43/500, Train Loss: 0.3519, Val Loss: 0.0136, Test Loss: 0.0165\n",
      "Epoch 44/500, Train Loss: 0.3371, Val Loss: 0.0217, Test Loss: 0.0250\n",
      "Epoch 45/500, Train Loss: 0.3209, Val Loss: 0.0374, Test Loss: 0.0416\n",
      "Epoch 46/500, Train Loss: 0.3299, Val Loss: 0.0408, Test Loss: 0.0457\n",
      "Epoch 47/500, Train Loss: 0.3426, Val Loss: 0.0332, Test Loss: 0.0380\n",
      "Epoch 48/500, Train Loss: 0.3185, Val Loss: 0.0191, Test Loss: 0.0233\n",
      "Epoch 49/500, Train Loss: 0.3081, Val Loss: 0.0156, Test Loss: 0.0193\n",
      "Epoch 50/500, Train Loss: 0.3206, Val Loss: 0.0148, Test Loss: 0.0180\n",
      "Epoch 51/500, Train Loss: 0.3078, Val Loss: 0.0142, Test Loss: 0.0172\n",
      "Epoch 52/500, Train Loss: 0.2958, Val Loss: 0.0137, Test Loss: 0.0168\n",
      "Epoch 53/500, Train Loss: 0.3396, Val Loss: 0.0139, Test Loss: 0.0170\n",
      "Epoch 54/500, Train Loss: 0.2591, Val Loss: 0.0147, Test Loss: 0.0179\n",
      "Epoch 55/500, Train Loss: 0.3015, Val Loss: 0.0164, Test Loss: 0.0197\n",
      "Epoch 56/500, Train Loss: 0.2966, Val Loss: 0.0198, Test Loss: 0.0233\n",
      "Epoch 57/500, Train Loss: 0.2477, Val Loss: 0.0189, Test Loss: 0.0222\n",
      "Epoch 58/500, Train Loss: 0.2798, Val Loss: 0.0161, Test Loss: 0.0188\n",
      "Epoch 59/500, Train Loss: 0.2577, Val Loss: 0.0124, Test Loss: 0.0146\n",
      "Epoch 60/500, Train Loss: 0.2701, Val Loss: 0.0119, Test Loss: 0.0138\n",
      "Epoch 61/500, Train Loss: 0.2810, Val Loss: 0.0128, Test Loss: 0.0148\n",
      "Epoch 62/500, Train Loss: 0.2370, Val Loss: 0.0145, Test Loss: 0.0166\n",
      "Epoch 63/500, Train Loss: 0.2808, Val Loss: 0.0180, Test Loss: 0.0201\n",
      "Epoch 64/500, Train Loss: 0.2637, Val Loss: 0.0181, Test Loss: 0.0204\n",
      "Epoch 65/500, Train Loss: 0.2593, Val Loss: 0.0167, Test Loss: 0.0189\n",
      "Epoch 66/500, Train Loss: 0.2593, Val Loss: 0.0150, Test Loss: 0.0170\n",
      "Epoch 67/500, Train Loss: 0.2386, Val Loss: 0.0138, Test Loss: 0.0159\n",
      "Epoch 68/500, Train Loss: 0.2568, Val Loss: 0.0119, Test Loss: 0.0142\n",
      "Epoch 69/500, Train Loss: 0.2472, Val Loss: 0.0111, Test Loss: 0.0135\n",
      "Epoch 70/500, Train Loss: 0.2681, Val Loss: 0.0109, Test Loss: 0.0135\n",
      "Epoch 71/500, Train Loss: 0.2473, Val Loss: 0.0116, Test Loss: 0.0142\n",
      "Epoch 72/500, Train Loss: 0.2472, Val Loss: 0.0134, Test Loss: 0.0158\n",
      "Epoch 73/500, Train Loss: 0.2401, Val Loss: 0.0161, Test Loss: 0.0185\n",
      "Epoch 74/500, Train Loss: 0.2651, Val Loss: 0.0140, Test Loss: 0.0166\n",
      "Epoch 75/500, Train Loss: 0.2242, Val Loss: 0.0127, Test Loss: 0.0155\n",
      "Epoch 76/500, Train Loss: 0.2368, Val Loss: 0.0123, Test Loss: 0.0150\n",
      "Epoch 77/500, Train Loss: 0.2015, Val Loss: 0.0130, Test Loss: 0.0155\n",
      "Epoch 78/500, Train Loss: 0.2171, Val Loss: 0.0130, Test Loss: 0.0153\n",
      "Epoch 79/500, Train Loss: 0.2331, Val Loss: 0.0123, Test Loss: 0.0145\n",
      "Epoch 80/500, Train Loss: 0.2327, Val Loss: 0.0122, Test Loss: 0.0143\n",
      "Epoch 81/500, Train Loss: 0.1985, Val Loss: 0.0118, Test Loss: 0.0137\n",
      "Epoch 82/500, Train Loss: 0.2011, Val Loss: 0.0132, Test Loss: 0.0150\n",
      "Epoch 83/500, Train Loss: 0.2410, Val Loss: 0.0119, Test Loss: 0.0135\n",
      "Epoch 84/500, Train Loss: 0.2049, Val Loss: 0.0113, Test Loss: 0.0128\n",
      "Epoch 85/500, Train Loss: 0.2028, Val Loss: 0.0107, Test Loss: 0.0123\n",
      "Epoch 86/500, Train Loss: 0.2132, Val Loss: 0.0103, Test Loss: 0.0119\n",
      "Epoch 87/500, Train Loss: 0.2000, Val Loss: 0.0101, Test Loss: 0.0116\n",
      "Epoch 88/500, Train Loss: 0.2081, Val Loss: 0.0105, Test Loss: 0.0121\n",
      "Epoch 89/500, Train Loss: 0.1879, Val Loss: 0.0129, Test Loss: 0.0145\n",
      "Epoch 90/500, Train Loss: 0.2017, Val Loss: 0.0158, Test Loss: 0.0175\n",
      "Epoch 91/500, Train Loss: 0.1826, Val Loss: 0.0187, Test Loss: 0.0205\n",
      "Epoch 92/500, Train Loss: 0.2015, Val Loss: 0.0177, Test Loss: 0.0195\n",
      "Epoch 93/500, Train Loss: 0.1903, Val Loss: 0.0138, Test Loss: 0.0159\n",
      "Epoch 94/500, Train Loss: 0.1818, Val Loss: 0.0096, Test Loss: 0.0123\n",
      "Epoch 95/500, Train Loss: 0.1816, Val Loss: 0.0089, Test Loss: 0.0122\n",
      "Epoch 96/500, Train Loss: 0.1927, Val Loss: 0.0095, Test Loss: 0.0132\n",
      "Epoch 97/500, Train Loss: 0.1810, Val Loss: 0.0089, Test Loss: 0.0127\n",
      "Epoch 98/500, Train Loss: 0.1868, Val Loss: 0.0084, Test Loss: 0.0121\n",
      "Epoch 99/500, Train Loss: 0.1882, Val Loss: 0.0091, Test Loss: 0.0126\n",
      "Epoch 100/500, Train Loss: 0.1490, Val Loss: 0.0127, Test Loss: 0.0158\n",
      "Epoch 101/500, Train Loss: 0.1796, Val Loss: 0.0185, Test Loss: 0.0211\n",
      "Epoch 102/500, Train Loss: 0.1669, Val Loss: 0.0199, Test Loss: 0.0223\n",
      "Epoch 103/500, Train Loss: 0.1756, Val Loss: 0.0192, Test Loss: 0.0213\n",
      "Epoch 104/500, Train Loss: 0.1561, Val Loss: 0.0159, Test Loss: 0.0181\n",
      "Epoch 105/500, Train Loss: 0.1756, Val Loss: 0.0115, Test Loss: 0.0140\n",
      "Epoch 106/500, Train Loss: 0.1757, Val Loss: 0.0087, Test Loss: 0.0115\n",
      "Epoch 107/500, Train Loss: 0.1747, Val Loss: 0.0076, Test Loss: 0.0108\n",
      "Epoch 108/500, Train Loss: 0.1557, Val Loss: 0.0081, Test Loss: 0.0117\n",
      "Epoch 109/500, Train Loss: 0.1605, Val Loss: 0.0076, Test Loss: 0.0109\n",
      "Epoch 110/500, Train Loss: 0.1570, Val Loss: 0.0076, Test Loss: 0.0104\n",
      "Epoch 111/500, Train Loss: 0.1935, Val Loss: 0.0105, Test Loss: 0.0128\n",
      "Epoch 112/500, Train Loss: 0.1811, Val Loss: 0.0159, Test Loss: 0.0178\n",
      "Epoch 113/500, Train Loss: 0.1533, Val Loss: 0.0191, Test Loss: 0.0209\n",
      "Epoch 114/500, Train Loss: 0.1630, Val Loss: 0.0163, Test Loss: 0.0182\n",
      "Epoch 115/500, Train Loss: 0.1480, Val Loss: 0.0097, Test Loss: 0.0120\n",
      "Epoch 116/500, Train Loss: 0.1638, Val Loss: 0.0078, Test Loss: 0.0106\n",
      "Epoch 117/500, Train Loss: 0.1507, Val Loss: 0.0093, Test Loss: 0.0125\n",
      "Epoch 118/500, Train Loss: 0.1581, Val Loss: 0.0093, Test Loss: 0.0125\n",
      "Epoch 119/500, Train Loss: 0.1582, Val Loss: 0.0087, Test Loss: 0.0118\n",
      "Epoch 120/500, Train Loss: 0.1482, Val Loss: 0.0085, Test Loss: 0.0111\n",
      "Epoch 121/500, Train Loss: 0.1369, Val Loss: 0.0107, Test Loss: 0.0129\n",
      "Epoch 122/500, Train Loss: 0.1345, Val Loss: 0.0137, Test Loss: 0.0156\n",
      "Epoch 123/500, Train Loss: 0.1607, Val Loss: 0.0134, Test Loss: 0.0152\n",
      "Epoch 124/500, Train Loss: 0.1379, Val Loss: 0.0114, Test Loss: 0.0134\n",
      "Epoch 125/500, Train Loss: 0.1584, Val Loss: 0.0096, Test Loss: 0.0116\n",
      "Epoch 126/500, Train Loss: 0.1555, Val Loss: 0.0094, Test Loss: 0.0113\n",
      "Epoch 127/500, Train Loss: 0.1292, Val Loss: 0.0089, Test Loss: 0.0107\n",
      "Epoch 128/500, Train Loss: 0.1289, Val Loss: 0.0088, Test Loss: 0.0105\n",
      "Epoch 129/500, Train Loss: 0.1469, Val Loss: 0.0089, Test Loss: 0.0104\n",
      "Epoch 130/500, Train Loss: 0.1396, Val Loss: 0.0091, Test Loss: 0.0105\n",
      "Epoch 131/500, Train Loss: 0.1433, Val Loss: 0.0093, Test Loss: 0.0105\n",
      "Epoch 132/500, Train Loss: 0.1216, Val Loss: 0.0087, Test Loss: 0.0101\n",
      "Epoch 133/500, Train Loss: 0.1345, Val Loss: 0.0083, Test Loss: 0.0100\n",
      "Epoch 134/500, Train Loss: 0.1290, Val Loss: 0.0081, Test Loss: 0.0100\n",
      "Epoch 135/500, Train Loss: 0.1324, Val Loss: 0.0086, Test Loss: 0.0102\n",
      "Epoch 136/500, Train Loss: 0.1266, Val Loss: 0.0091, Test Loss: 0.0107\n",
      "Epoch 137/500, Train Loss: 0.1399, Val Loss: 0.0110, Test Loss: 0.0123\n",
      "Epoch 138/500, Train Loss: 0.1423, Val Loss: 0.0134, Test Loss: 0.0145\n",
      "Epoch 139/500, Train Loss: 0.1298, Val Loss: 0.0148, Test Loss: 0.0158\n",
      "Epoch 140/500, Train Loss: 0.1230, Val Loss: 0.0130, Test Loss: 0.0143\n",
      "Epoch 141/500, Train Loss: 0.1215, Val Loss: 0.0090, Test Loss: 0.0108\n",
      "Epoch 142/500, Train Loss: 0.1220, Val Loss: 0.0075, Test Loss: 0.0098\n",
      "Epoch 143/500, Train Loss: 0.1188, Val Loss: 0.0075, Test Loss: 0.0101\n",
      "Epoch 144/500, Train Loss: 0.1281, Val Loss: 0.0078, Test Loss: 0.0104\n",
      "Epoch 145/500, Train Loss: 0.1132, Val Loss: 0.0075, Test Loss: 0.0100\n",
      "Epoch 146/500, Train Loss: 0.1103, Val Loss: 0.0075, Test Loss: 0.0096\n",
      "Epoch 147/500, Train Loss: 0.1274, Val Loss: 0.0090, Test Loss: 0.0107\n",
      "Epoch 148/500, Train Loss: 0.1098, Val Loss: 0.0119, Test Loss: 0.0134\n",
      "Epoch 149/500, Train Loss: 0.1229, Val Loss: 0.0134, Test Loss: 0.0148\n",
      "Epoch 150/500, Train Loss: 0.1149, Val Loss: 0.0119, Test Loss: 0.0136\n",
      "Epoch 151/500, Train Loss: 0.1235, Val Loss: 0.0099, Test Loss: 0.0118\n",
      "Epoch 152/500, Train Loss: 0.1084, Val Loss: 0.0081, Test Loss: 0.0103\n",
      "Epoch 153/500, Train Loss: 0.1132, Val Loss: 0.0071, Test Loss: 0.0097\n",
      "Epoch 154/500, Train Loss: 0.1266, Val Loss: 0.0073, Test Loss: 0.0102\n",
      "Epoch 155/500, Train Loss: 0.1198, Val Loss: 0.0075, Test Loss: 0.0105\n",
      "Epoch 156/500, Train Loss: 0.1256, Val Loss: 0.0073, Test Loss: 0.0102\n",
      "Epoch 157/500, Train Loss: 0.1204, Val Loss: 0.0075, Test Loss: 0.0102\n",
      "Epoch 158/500, Train Loss: 0.1065, Val Loss: 0.0082, Test Loss: 0.0107\n",
      "Epoch 159/500, Train Loss: 0.1145, Val Loss: 0.0089, Test Loss: 0.0112\n",
      "Epoch 160/500, Train Loss: 0.1070, Val Loss: 0.0089, Test Loss: 0.0113\n",
      "Epoch 161/500, Train Loss: 0.1226, Val Loss: 0.0091, Test Loss: 0.0115\n",
      "Epoch 162/500, Train Loss: 0.1094, Val Loss: 0.0089, Test Loss: 0.0113\n",
      "Epoch 163/500, Train Loss: 0.1017, Val Loss: 0.0087, Test Loss: 0.0112\n",
      "Epoch 164/500, Train Loss: 0.1085, Val Loss: 0.0083, Test Loss: 0.0109\n",
      "Epoch 165/500, Train Loss: 0.1025, Val Loss: 0.0083, Test Loss: 0.0107\n",
      "Epoch 166/500, Train Loss: 0.1097, Val Loss: 0.0081, Test Loss: 0.0105\n",
      "Epoch 167/500, Train Loss: 0.1137, Val Loss: 0.0079, Test Loss: 0.0102\n",
      "Epoch 168/500, Train Loss: 0.1061, Val Loss: 0.0078, Test Loss: 0.0100\n",
      "Epoch 169/500, Train Loss: 0.1066, Val Loss: 0.0079, Test Loss: 0.0099\n",
      "Epoch 170/500, Train Loss: 0.1017, Val Loss: 0.0081, Test Loss: 0.0099\n",
      "Epoch 171/500, Train Loss: 0.1004, Val Loss: 0.0092, Test Loss: 0.0106\n",
      "Epoch 172/500, Train Loss: 0.1076, Val Loss: 0.0091, Test Loss: 0.0106\n",
      "Epoch 173/500, Train Loss: 0.1008, Val Loss: 0.0082, Test Loss: 0.0099\n",
      "Epoch 174/500, Train Loss: 0.0933, Val Loss: 0.0077, Test Loss: 0.0097\n",
      "Epoch 175/500, Train Loss: 0.1082, Val Loss: 0.0073, Test Loss: 0.0095\n",
      "Epoch 176/500, Train Loss: 0.0971, Val Loss: 0.0071, Test Loss: 0.0094\n",
      "Epoch 177/500, Train Loss: 0.1059, Val Loss: 0.0069, Test Loss: 0.0094\n",
      "Epoch 178/500, Train Loss: 0.1029, Val Loss: 0.0068, Test Loss: 0.0093\n",
      "Epoch 179/500, Train Loss: 0.0958, Val Loss: 0.0068, Test Loss: 0.0092\n",
      "Epoch 180/500, Train Loss: 0.0954, Val Loss: 0.0075, Test Loss: 0.0095\n",
      "Epoch 181/500, Train Loss: 0.1036, Val Loss: 0.0087, Test Loss: 0.0104\n",
      "Epoch 182/500, Train Loss: 0.0975, Val Loss: 0.0090, Test Loss: 0.0107\n",
      "Epoch 183/500, Train Loss: 0.1025, Val Loss: 0.0080, Test Loss: 0.0098\n",
      "Epoch 184/500, Train Loss: 0.0996, Val Loss: 0.0073, Test Loss: 0.0093\n",
      "Epoch 185/500, Train Loss: 0.0883, Val Loss: 0.0069, Test Loss: 0.0089\n",
      "Epoch 186/500, Train Loss: 0.0944, Val Loss: 0.0068, Test Loss: 0.0089\n",
      "Epoch 187/500, Train Loss: 0.0909, Val Loss: 0.0068, Test Loss: 0.0090\n",
      "Epoch 188/500, Train Loss: 0.0880, Val Loss: 0.0069, Test Loss: 0.0088\n",
      "Epoch 189/500, Train Loss: 0.0970, Val Loss: 0.0075, Test Loss: 0.0091\n",
      "Epoch 190/500, Train Loss: 0.0907, Val Loss: 0.0082, Test Loss: 0.0097\n",
      "Epoch 191/500, Train Loss: 0.1030, Val Loss: 0.0082, Test Loss: 0.0097\n",
      "Epoch 192/500, Train Loss: 0.0925, Val Loss: 0.0076, Test Loss: 0.0092\n",
      "Epoch 193/500, Train Loss: 0.0845, Val Loss: 0.0069, Test Loss: 0.0088\n",
      "Epoch 194/500, Train Loss: 0.0969, Val Loss: 0.0068, Test Loss: 0.0087\n",
      "Epoch 195/500, Train Loss: 0.0894, Val Loss: 0.0072, Test Loss: 0.0090\n",
      "Epoch 196/500, Train Loss: 0.0921, Val Loss: 0.0076, Test Loss: 0.0094\n",
      "Epoch 197/500, Train Loss: 0.0945, Val Loss: 0.0083, Test Loss: 0.0101\n",
      "Epoch 198/500, Train Loss: 0.0930, Val Loss: 0.0087, Test Loss: 0.0106\n",
      "Epoch 199/500, Train Loss: 0.0894, Val Loss: 0.0077, Test Loss: 0.0097\n",
      "Epoch 200/500, Train Loss: 0.0881, Val Loss: 0.0071, Test Loss: 0.0093\n",
      "Epoch 201/500, Train Loss: 0.0873, Val Loss: 0.0069, Test Loss: 0.0092\n",
      "Epoch 202/500, Train Loss: 0.0844, Val Loss: 0.0068, Test Loss: 0.0091\n",
      "Epoch 203/500, Train Loss: 0.0937, Val Loss: 0.0069, Test Loss: 0.0092\n",
      "Epoch 204/500, Train Loss: 0.0884, Val Loss: 0.0072, Test Loss: 0.0095\n",
      "Epoch 205/500, Train Loss: 0.0744, Val Loss: 0.0074, Test Loss: 0.0097\n",
      "Epoch 206/500, Train Loss: 0.0786, Val Loss: 0.0074, Test Loss: 0.0097\n",
      "Epoch 207/500, Train Loss: 0.0923, Val Loss: 0.0075, Test Loss: 0.0098\n",
      "Epoch 208/500, Train Loss: 0.0795, Val Loss: 0.0084, Test Loss: 0.0107\n",
      "Epoch 209/500, Train Loss: 0.0862, Val Loss: 0.0082, Test Loss: 0.0106\n",
      "Epoch 210/500, Train Loss: 0.0925, Val Loss: 0.0076, Test Loss: 0.0100\n",
      "Epoch 211/500, Train Loss: 0.0727, Val Loss: 0.0070, Test Loss: 0.0097\n",
      "Epoch 212/500, Train Loss: 0.0859, Val Loss: 0.0072, Test Loss: 0.0102\n",
      "Epoch 213/500, Train Loss: 0.0864, Val Loss: 0.0074, Test Loss: 0.0105\n",
      "Epoch 214/500, Train Loss: 0.0826, Val Loss: 0.0072, Test Loss: 0.0102\n",
      "Epoch 215/500, Train Loss: 0.0828, Val Loss: 0.0074, Test Loss: 0.0101\n",
      "Epoch 216/500, Train Loss: 0.0760, Val Loss: 0.0082, Test Loss: 0.0106\n",
      "Epoch 217/500, Train Loss: 0.0791, Val Loss: 0.0099, Test Loss: 0.0119\n",
      "Epoch 218/500, Train Loss: 0.0821, Val Loss: 0.0095, Test Loss: 0.0115\n",
      "Epoch 219/500, Train Loss: 0.0844, Val Loss: 0.0081, Test Loss: 0.0104\n",
      "Epoch 220/500, Train Loss: 0.0744, Val Loss: 0.0072, Test Loss: 0.0098\n",
      "Epoch 221/500, Train Loss: 0.0797, Val Loss: 0.0071, Test Loss: 0.0100\n",
      "Epoch 222/500, Train Loss: 0.0857, Val Loss: 0.0074, Test Loss: 0.0105\n",
      "Epoch 223/500, Train Loss: 0.0650, Val Loss: 0.0075, Test Loss: 0.0106\n",
      "Epoch 224/500, Train Loss: 0.0831, Val Loss: 0.0071, Test Loss: 0.0099\n",
      "Epoch 225/500, Train Loss: 0.0858, Val Loss: 0.0075, Test Loss: 0.0098\n",
      "Epoch 226/500, Train Loss: 0.0744, Val Loss: 0.0094, Test Loss: 0.0112\n",
      "Epoch 227/500, Train Loss: 0.0783, Val Loss: 0.0107, Test Loss: 0.0123\n",
      "Epoch 228/500, Train Loss: 0.0808, Val Loss: 0.0099, Test Loss: 0.0115\n",
      "Epoch 229/500, Train Loss: 0.0697, Val Loss: 0.0081, Test Loss: 0.0099\n",
      "Epoch 230/500, Train Loss: 0.0738, Val Loss: 0.0068, Test Loss: 0.0090\n",
      "Epoch 231/500, Train Loss: 0.0818, Val Loss: 0.0071, Test Loss: 0.0097\n",
      "Epoch 232/500, Train Loss: 0.0802, Val Loss: 0.0077, Test Loss: 0.0106\n",
      "Epoch 233/500, Train Loss: 0.0758, Val Loss: 0.0073, Test Loss: 0.0100\n",
      "Epoch 234/500, Train Loss: 0.0665, Val Loss: 0.0066, Test Loss: 0.0090\n",
      "Epoch 235/500, Train Loss: 0.0745, Val Loss: 0.0074, Test Loss: 0.0094\n",
      "Epoch 236/500, Train Loss: 0.0696, Val Loss: 0.0099, Test Loss: 0.0116\n",
      "Epoch 237/500, Train Loss: 0.0725, Val Loss: 0.0109, Test Loss: 0.0126\n",
      "Epoch 238/500, Train Loss: 0.0709, Val Loss: 0.0099, Test Loss: 0.0117\n",
      "Epoch 239/500, Train Loss: 0.0696, Val Loss: 0.0081, Test Loss: 0.0102\n",
      "Epoch 240/500, Train Loss: 0.0642, Val Loss: 0.0073, Test Loss: 0.0097\n",
      "Epoch 241/500, Train Loss: 0.0693, Val Loss: 0.0072, Test Loss: 0.0099\n",
      "Epoch 242/500, Train Loss: 0.0686, Val Loss: 0.0074, Test Loss: 0.0102\n",
      "Epoch 243/500, Train Loss: 0.0626, Val Loss: 0.0074, Test Loss: 0.0104\n",
      "Epoch 244/500, Train Loss: 0.0738, Val Loss: 0.0073, Test Loss: 0.0102\n",
      "Epoch 245/500, Train Loss: 0.0679, Val Loss: 0.0072, Test Loss: 0.0101\n",
      "Epoch 246/500, Train Loss: 0.0712, Val Loss: 0.0076, Test Loss: 0.0102\n",
      "Epoch 247/500, Train Loss: 0.0693, Val Loss: 0.0086, Test Loss: 0.0109\n",
      "Epoch 248/500, Train Loss: 0.0665, Val Loss: 0.0092, Test Loss: 0.0113\n",
      "Epoch 249/500, Train Loss: 0.0691, Val Loss: 0.0085, Test Loss: 0.0107\n",
      "Epoch 250/500, Train Loss: 0.0713, Val Loss: 0.0078, Test Loss: 0.0101\n",
      "Epoch 251/500, Train Loss: 0.0706, Val Loss: 0.0071, Test Loss: 0.0095\n",
      "Epoch 252/500, Train Loss: 0.0669, Val Loss: 0.0068, Test Loss: 0.0093\n",
      "Epoch 253/500, Train Loss: 0.0726, Val Loss: 0.0067, Test Loss: 0.0094\n",
      "Epoch 254/500, Train Loss: 0.0649, Val Loss: 0.0067, Test Loss: 0.0094\n",
      "Epoch 255/500, Train Loss: 0.0663, Val Loss: 0.0066, Test Loss: 0.0092\n",
      "Epoch 256/500, Train Loss: 0.0661, Val Loss: 0.0071, Test Loss: 0.0093\n",
      "Epoch 257/500, Train Loss: 0.0645, Val Loss: 0.0084, Test Loss: 0.0102\n",
      "Epoch 258/500, Train Loss: 0.0618, Val Loss: 0.0089, Test Loss: 0.0106\n",
      "Epoch 259/500, Train Loss: 0.0668, Val Loss: 0.0086, Test Loss: 0.0104\n",
      "Epoch 260/500, Train Loss: 0.0741, Val Loss: 0.0078, Test Loss: 0.0098\n",
      "Epoch 261/500, Train Loss: 0.0613, Val Loss: 0.0072, Test Loss: 0.0097\n",
      "Epoch 262/500, Train Loss: 0.0628, Val Loss: 0.0074, Test Loss: 0.0103\n",
      "Epoch 263/500, Train Loss: 0.0626, Val Loss: 0.0074, Test Loss: 0.0103\n",
      "Epoch 264/500, Train Loss: 0.0685, Val Loss: 0.0074, Test Loss: 0.0102\n",
      "Epoch 265/500, Train Loss: 0.0575, Val Loss: 0.0072, Test Loss: 0.0098\n",
      "Epoch 266/500, Train Loss: 0.0619, Val Loss: 0.0076, Test Loss: 0.0098\n",
      "Epoch 267/500, Train Loss: 0.0592, Val Loss: 0.0090, Test Loss: 0.0106\n",
      "Epoch 268/500, Train Loss: 0.0676, Val Loss: 0.0097, Test Loss: 0.0112\n",
      "Epoch 269/500, Train Loss: 0.0636, Val Loss: 0.0098, Test Loss: 0.0112\n",
      "Epoch 270/500, Train Loss: 0.0657, Val Loss: 0.0083, Test Loss: 0.0100\n",
      "Epoch 271/500, Train Loss: 0.0575, Val Loss: 0.0072, Test Loss: 0.0093\n",
      "Epoch 272/500, Train Loss: 0.0606, Val Loss: 0.0069, Test Loss: 0.0095\n",
      "Epoch 273/500, Train Loss: 0.0580, Val Loss: 0.0070, Test Loss: 0.0098\n",
      "Epoch 274/500, Train Loss: 0.0625, Val Loss: 0.0067, Test Loss: 0.0094\n",
      "Epoch 275/500, Train Loss: 0.0563, Val Loss: 0.0065, Test Loss: 0.0088\n",
      "Epoch 276/500, Train Loss: 0.0597, Val Loss: 0.0069, Test Loss: 0.0089\n",
      "Epoch 277/500, Train Loss: 0.0596, Val Loss: 0.0081, Test Loss: 0.0098\n",
      "Epoch 278/500, Train Loss: 0.0578, Val Loss: 0.0080, Test Loss: 0.0097\n",
      "Epoch 279/500, Train Loss: 0.0614, Val Loss: 0.0074, Test Loss: 0.0093\n",
      "Epoch 280/500, Train Loss: 0.0594, Val Loss: 0.0065, Test Loss: 0.0087\n",
      "Epoch 281/500, Train Loss: 0.0625, Val Loss: 0.0062, Test Loss: 0.0086\n",
      "Epoch 282/500, Train Loss: 0.0578, Val Loss: 0.0062, Test Loss: 0.0087\n",
      "Epoch 283/500, Train Loss: 0.0593, Val Loss: 0.0062, Test Loss: 0.0089\n",
      "Epoch 284/500, Train Loss: 0.0557, Val Loss: 0.0061, Test Loss: 0.0087\n",
      "Epoch 285/500, Train Loss: 0.0524, Val Loss: 0.0062, Test Loss: 0.0087\n",
      "Epoch 286/500, Train Loss: 0.0602, Val Loss: 0.0062, Test Loss: 0.0087\n",
      "Epoch 287/500, Train Loss: 0.0544, Val Loss: 0.0064, Test Loss: 0.0088\n",
      "Epoch 288/500, Train Loss: 0.0637, Val Loss: 0.0064, Test Loss: 0.0088\n",
      "Epoch 289/500, Train Loss: 0.0627, Val Loss: 0.0063, Test Loss: 0.0088\n",
      "Epoch 290/500, Train Loss: 0.0543, Val Loss: 0.0063, Test Loss: 0.0088\n",
      "Epoch 291/500, Train Loss: 0.0499, Val Loss: 0.0062, Test Loss: 0.0088\n",
      "Epoch 292/500, Train Loss: 0.0558, Val Loss: 0.0062, Test Loss: 0.0087\n",
      "Epoch 293/500, Train Loss: 0.0582, Val Loss: 0.0063, Test Loss: 0.0087\n",
      "Epoch 294/500, Train Loss: 0.0519, Val Loss: 0.0063, Test Loss: 0.0086\n",
      "Epoch 295/500, Train Loss: 0.0495, Val Loss: 0.0062, Test Loss: 0.0086\n",
      "Epoch 296/500, Train Loss: 0.0594, Val Loss: 0.0062, Test Loss: 0.0085\n",
      "Epoch 297/500, Train Loss: 0.0493, Val Loss: 0.0062, Test Loss: 0.0085\n",
      "Epoch 298/500, Train Loss: 0.0550, Val Loss: 0.0061, Test Loss: 0.0085\n",
      "Epoch 299/500, Train Loss: 0.0578, Val Loss: 0.0060, Test Loss: 0.0084\n",
      "Epoch 300/500, Train Loss: 0.0540, Val Loss: 0.0061, Test Loss: 0.0085\n",
      "Epoch 301/500, Train Loss: 0.0576, Val Loss: 0.0061, Test Loss: 0.0085\n",
      "Epoch 302/500, Train Loss: 0.0546, Val Loss: 0.0061, Test Loss: 0.0085\n",
      "Epoch 303/500, Train Loss: 0.0544, Val Loss: 0.0062, Test Loss: 0.0085\n",
      "Epoch 304/500, Train Loss: 0.0554, Val Loss: 0.0064, Test Loss: 0.0086\n",
      "Epoch 305/500, Train Loss: 0.0533, Val Loss: 0.0067, Test Loss: 0.0089\n",
      "Epoch 306/500, Train Loss: 0.0523, Val Loss: 0.0067, Test Loss: 0.0089\n",
      "Epoch 307/500, Train Loss: 0.0530, Val Loss: 0.0066, Test Loss: 0.0089\n",
      "Epoch 308/500, Train Loss: 0.0477, Val Loss: 0.0066, Test Loss: 0.0089\n",
      "Epoch 309/500, Train Loss: 0.0500, Val Loss: 0.0066, Test Loss: 0.0090\n",
      "Epoch 310/500, Train Loss: 0.0572, Val Loss: 0.0067, Test Loss: 0.0090\n",
      "Epoch 311/500, Train Loss: 0.0582, Val Loss: 0.0068, Test Loss: 0.0091\n",
      "Epoch 312/500, Train Loss: 0.0558, Val Loss: 0.0070, Test Loss: 0.0092\n",
      "Epoch 313/500, Train Loss: 0.0519, Val Loss: 0.0071, Test Loss: 0.0092\n",
      "Epoch 314/500, Train Loss: 0.0504, Val Loss: 0.0070, Test Loss: 0.0092\n",
      "Epoch 315/500, Train Loss: 0.0571, Val Loss: 0.0068, Test Loss: 0.0092\n",
      "Epoch 316/500, Train Loss: 0.0561, Val Loss: 0.0068, Test Loss: 0.0093\n",
      "Epoch 317/500, Train Loss: 0.0522, Val Loss: 0.0068, Test Loss: 0.0093\n",
      "Epoch 318/500, Train Loss: 0.0501, Val Loss: 0.0068, Test Loss: 0.0092\n",
      "Epoch 319/500, Train Loss: 0.0543, Val Loss: 0.0071, Test Loss: 0.0092\n",
      "Epoch 320/500, Train Loss: 0.0474, Val Loss: 0.0075, Test Loss: 0.0094\n",
      "Epoch 321/500, Train Loss: 0.0514, Val Loss: 0.0074, Test Loss: 0.0092\n",
      "Epoch 322/500, Train Loss: 0.0500, Val Loss: 0.0068, Test Loss: 0.0087\n",
      "Epoch 323/500, Train Loss: 0.0445, Val Loss: 0.0064, Test Loss: 0.0086\n",
      "Epoch 324/500, Train Loss: 0.0493, Val Loss: 0.0064, Test Loss: 0.0087\n",
      "Epoch 325/500, Train Loss: 0.0531, Val Loss: 0.0064, Test Loss: 0.0088\n",
      "Epoch 326/500, Train Loss: 0.0503, Val Loss: 0.0063, Test Loss: 0.0085\n",
      "Epoch 327/500, Train Loss: 0.0493, Val Loss: 0.0061, Test Loss: 0.0082\n",
      "Epoch 328/500, Train Loss: 0.0483, Val Loss: 0.0068, Test Loss: 0.0085\n",
      "Epoch 329/500, Train Loss: 0.0490, Val Loss: 0.0080, Test Loss: 0.0094\n",
      "Epoch 330/500, Train Loss: 0.0522, Val Loss: 0.0081, Test Loss: 0.0096\n",
      "Epoch 331/500, Train Loss: 0.0524, Val Loss: 0.0071, Test Loss: 0.0087\n",
      "Epoch 332/500, Train Loss: 0.0511, Val Loss: 0.0062, Test Loss: 0.0082\n",
      "Epoch 333/500, Train Loss: 0.0551, Val Loss: 0.0061, Test Loss: 0.0084\n",
      "Epoch 334/500, Train Loss: 0.0479, Val Loss: 0.0063, Test Loss: 0.0088\n",
      "Epoch 335/500, Train Loss: 0.0478, Val Loss: 0.0062, Test Loss: 0.0086\n",
      "Epoch 336/500, Train Loss: 0.0487, Val Loss: 0.0066, Test Loss: 0.0087\n",
      "Epoch 337/500, Train Loss: 0.0459, Val Loss: 0.0074, Test Loss: 0.0094\n",
      "Epoch 338/500, Train Loss: 0.0470, Val Loss: 0.0076, Test Loss: 0.0096\n",
      "Epoch 339/500, Train Loss: 0.0467, Val Loss: 0.0070, Test Loss: 0.0092\n",
      "Epoch 340/500, Train Loss: 0.0465, Val Loss: 0.0066, Test Loss: 0.0089\n",
      "Epoch 341/500, Train Loss: 0.0459, Val Loss: 0.0064, Test Loss: 0.0090\n",
      "Epoch 342/500, Train Loss: 0.0506, Val Loss: 0.0065, Test Loss: 0.0091\n",
      "Epoch 343/500, Train Loss: 0.0487, Val Loss: 0.0065, Test Loss: 0.0091\n",
      "Epoch 344/500, Train Loss: 0.0467, Val Loss: 0.0065, Test Loss: 0.0091\n",
      "Epoch 345/500, Train Loss: 0.0443, Val Loss: 0.0066, Test Loss: 0.0091\n",
      "Epoch 346/500, Train Loss: 0.0475, Val Loss: 0.0069, Test Loss: 0.0093\n",
      "Epoch 347/500, Train Loss: 0.0433, Val Loss: 0.0069, Test Loss: 0.0093\n",
      "Epoch 348/500, Train Loss: 0.0462, Val Loss: 0.0068, Test Loss: 0.0092\n",
      "Epoch 349/500, Train Loss: 0.0448, Val Loss: 0.0066, Test Loss: 0.0091\n",
      "Epoch 350/500, Train Loss: 0.0485, Val Loss: 0.0066, Test Loss: 0.0091\n",
      "Epoch 351/500, Train Loss: 0.0433, Val Loss: 0.0065, Test Loss: 0.0090\n",
      "Epoch 352/500, Train Loss: 0.0447, Val Loss: 0.0065, Test Loss: 0.0090\n",
      "Epoch 353/500, Train Loss: 0.0451, Val Loss: 0.0064, Test Loss: 0.0088\n",
      "Epoch 354/500, Train Loss: 0.0458, Val Loss: 0.0065, Test Loss: 0.0088\n",
      "Epoch 355/500, Train Loss: 0.0461, Val Loss: 0.0065, Test Loss: 0.0087\n",
      "Epoch 356/500, Train Loss: 0.0452, Val Loss: 0.0065, Test Loss: 0.0087\n",
      "Epoch 357/500, Train Loss: 0.0447, Val Loss: 0.0065, Test Loss: 0.0087\n",
      "Epoch 358/500, Train Loss: 0.0480, Val Loss: 0.0064, Test Loss: 0.0086\n",
      "Epoch 359/500, Train Loss: 0.0440, Val Loss: 0.0064, Test Loss: 0.0086\n",
      "Epoch 360/500, Train Loss: 0.0466, Val Loss: 0.0064, Test Loss: 0.0085\n",
      "Epoch 361/500, Train Loss: 0.0441, Val Loss: 0.0063, Test Loss: 0.0085\n",
      "Epoch 362/500, Train Loss: 0.0458, Val Loss: 0.0063, Test Loss: 0.0084\n",
      "Epoch 363/500, Train Loss: 0.0432, Val Loss: 0.0065, Test Loss: 0.0084\n",
      "Epoch 364/500, Train Loss: 0.0427, Val Loss: 0.0066, Test Loss: 0.0085\n",
      "Epoch 365/500, Train Loss: 0.0439, Val Loss: 0.0066, Test Loss: 0.0085\n",
      "Epoch 366/500, Train Loss: 0.0458, Val Loss: 0.0065, Test Loss: 0.0084\n",
      "Epoch 367/500, Train Loss: 0.0441, Val Loss: 0.0063, Test Loss: 0.0084\n",
      "Epoch 368/500, Train Loss: 0.0437, Val Loss: 0.0064, Test Loss: 0.0086\n",
      "Epoch 369/500, Train Loss: 0.0453, Val Loss: 0.0064, Test Loss: 0.0087\n",
      "Epoch 370/500, Train Loss: 0.0373, Val Loss: 0.0064, Test Loss: 0.0087\n",
      "Epoch 371/500, Train Loss: 0.0436, Val Loss: 0.0065, Test Loss: 0.0088\n",
      "Epoch 372/500, Train Loss: 0.0438, Val Loss: 0.0066, Test Loss: 0.0088\n",
      "Epoch 373/500, Train Loss: 0.0457, Val Loss: 0.0068, Test Loss: 0.0090\n",
      "Epoch 374/500, Train Loss: 0.0403, Val Loss: 0.0068, Test Loss: 0.0091\n",
      "Epoch 375/500, Train Loss: 0.0411, Val Loss: 0.0068, Test Loss: 0.0091\n",
      "Epoch 376/500, Train Loss: 0.0417, Val Loss: 0.0068, Test Loss: 0.0091\n",
      "Epoch 377/500, Train Loss: 0.0435, Val Loss: 0.0068, Test Loss: 0.0091\n",
      "Epoch 378/500, Train Loss: 0.0429, Val Loss: 0.0067, Test Loss: 0.0091\n",
      "Epoch 379/500, Train Loss: 0.0374, Val Loss: 0.0067, Test Loss: 0.0091\n",
      "Epoch 380/500, Train Loss: 0.0436, Val Loss: 0.0068, Test Loss: 0.0091\n",
      "Epoch 381/500, Train Loss: 0.0394, Val Loss: 0.0068, Test Loss: 0.0091\n",
      "Epoch 382/500, Train Loss: 0.0400, Val Loss: 0.0069, Test Loss: 0.0091\n",
      "Epoch 383/500, Train Loss: 0.0438, Val Loss: 0.0067, Test Loss: 0.0090\n",
      "Epoch 384/500, Train Loss: 0.0420, Val Loss: 0.0066, Test Loss: 0.0089\n",
      "Epoch 385/500, Train Loss: 0.0441, Val Loss: 0.0065, Test Loss: 0.0088\n",
      "Epoch 386/500, Train Loss: 0.0448, Val Loss: 0.0064, Test Loss: 0.0088\n",
      "Epoch 387/500, Train Loss: 0.0392, Val Loss: 0.0063, Test Loss: 0.0087\n",
      "Epoch 388/500, Train Loss: 0.0431, Val Loss: 0.0063, Test Loss: 0.0086\n",
      "Epoch 389/500, Train Loss: 0.0396, Val Loss: 0.0063, Test Loss: 0.0085\n",
      "Epoch 390/500, Train Loss: 0.0420, Val Loss: 0.0062, Test Loss: 0.0084\n",
      "Epoch 391/500, Train Loss: 0.0410, Val Loss: 0.0063, Test Loss: 0.0084\n",
      "Epoch 392/500, Train Loss: 0.0399, Val Loss: 0.0064, Test Loss: 0.0083\n",
      "Epoch 393/500, Train Loss: 0.0379, Val Loss: 0.0063, Test Loss: 0.0083\n",
      "Epoch 394/500, Train Loss: 0.0403, Val Loss: 0.0062, Test Loss: 0.0082\n",
      "Epoch 395/500, Train Loss: 0.0389, Val Loss: 0.0062, Test Loss: 0.0082\n",
      "Epoch 396/500, Train Loss: 0.0428, Val Loss: 0.0061, Test Loss: 0.0082\n",
      "Epoch 397/500, Train Loss: 0.0360, Val Loss: 0.0061, Test Loss: 0.0084\n",
      "Epoch 398/500, Train Loss: 0.0349, Val Loss: 0.0061, Test Loss: 0.0085\n",
      "Epoch 399/500, Train Loss: 0.0392, Val Loss: 0.0061, Test Loss: 0.0083\n",
      "Epoch 400/500, Train Loss: 0.0392, Val Loss: 0.0062, Test Loss: 0.0082\n",
      "Epoch 401/500, Train Loss: 0.0387, Val Loss: 0.0066, Test Loss: 0.0084\n",
      "Epoch 402/500, Train Loss: 0.0363, Val Loss: 0.0067, Test Loss: 0.0085\n",
      "Epoch 403/500, Train Loss: 0.0415, Val Loss: 0.0066, Test Loss: 0.0084\n",
      "Epoch 404/500, Train Loss: 0.0407, Val Loss: 0.0064, Test Loss: 0.0083\n",
      "Epoch 405/500, Train Loss: 0.0431, Val Loss: 0.0063, Test Loss: 0.0082\n",
      "Epoch 406/500, Train Loss: 0.0357, Val Loss: 0.0061, Test Loss: 0.0082\n",
      "Epoch 407/500, Train Loss: 0.0400, Val Loss: 0.0061, Test Loss: 0.0082\n",
      "Epoch 408/500, Train Loss: 0.0410, Val Loss: 0.0061, Test Loss: 0.0082\n",
      "Epoch 409/500, Train Loss: 0.0354, Val Loss: 0.0062, Test Loss: 0.0082\n",
      "Epoch 410/500, Train Loss: 0.0386, Val Loss: 0.0062, Test Loss: 0.0082\n",
      "Epoch 411/500, Train Loss: 0.0360, Val Loss: 0.0061, Test Loss: 0.0081\n",
      "Epoch 412/500, Train Loss: 0.0451, Val Loss: 0.0060, Test Loss: 0.0082\n",
      "Epoch 413/500, Train Loss: 0.0413, Val Loss: 0.0060, Test Loss: 0.0082\n",
      "Epoch 414/500, Train Loss: 0.0375, Val Loss: 0.0061, Test Loss: 0.0082\n",
      "Epoch 415/500, Train Loss: 0.0376, Val Loss: 0.0062, Test Loss: 0.0083\n",
      "Epoch 416/500, Train Loss: 0.0401, Val Loss: 0.0063, Test Loss: 0.0083\n",
      "Epoch 417/500, Train Loss: 0.0365, Val Loss: 0.0064, Test Loss: 0.0084\n",
      "Epoch 418/500, Train Loss: 0.0355, Val Loss: 0.0065, Test Loss: 0.0085\n",
      "Epoch 419/500, Train Loss: 0.0379, Val Loss: 0.0065, Test Loss: 0.0086\n",
      "Epoch 420/500, Train Loss: 0.0390, Val Loss: 0.0065, Test Loss: 0.0087\n",
      "Epoch 421/500, Train Loss: 0.0396, Val Loss: 0.0066, Test Loss: 0.0090\n",
      "Epoch 422/500, Train Loss: 0.0352, Val Loss: 0.0068, Test Loss: 0.0093\n",
      "Epoch 423/500, Train Loss: 0.0339, Val Loss: 0.0068, Test Loss: 0.0093\n",
      "Epoch 424/500, Train Loss: 0.0370, Val Loss: 0.0067, Test Loss: 0.0091\n",
      "Epoch 425/500, Train Loss: 0.0373, Val Loss: 0.0069, Test Loss: 0.0091\n",
      "Epoch 426/500, Train Loss: 0.0403, Val Loss: 0.0072, Test Loss: 0.0092\n",
      "Epoch 427/500, Train Loss: 0.0378, Val Loss: 0.0076, Test Loss: 0.0095\n",
      "Epoch 428/500, Train Loss: 0.0372, Val Loss: 0.0078, Test Loss: 0.0096\n",
      "Epoch 429/500, Train Loss: 0.0365, Val Loss: 0.0072, Test Loss: 0.0092\n",
      "Epoch 430/500, Train Loss: 0.0365, Val Loss: 0.0069, Test Loss: 0.0090\n",
      "Epoch 431/500, Train Loss: 0.0338, Val Loss: 0.0068, Test Loss: 0.0089\n",
      "Epoch 432/500, Train Loss: 0.0327, Val Loss: 0.0067, Test Loss: 0.0089\n",
      "Epoch 433/500, Train Loss: 0.0380, Val Loss: 0.0066, Test Loss: 0.0089\n",
      "Epoch 434/500, Train Loss: 0.0360, Val Loss: 0.0066, Test Loss: 0.0088\n",
      "Epoch 435/500, Train Loss: 0.0352, Val Loss: 0.0065, Test Loss: 0.0087\n",
      "Epoch 436/500, Train Loss: 0.0334, Val Loss: 0.0065, Test Loss: 0.0087\n",
      "Epoch 437/500, Train Loss: 0.0329, Val Loss: 0.0064, Test Loss: 0.0086\n",
      "Epoch 438/500, Train Loss: 0.0320, Val Loss: 0.0063, Test Loss: 0.0086\n",
      "Epoch 439/500, Train Loss: 0.0370, Val Loss: 0.0063, Test Loss: 0.0086\n",
      "Epoch 440/500, Train Loss: 0.0367, Val Loss: 0.0063, Test Loss: 0.0084\n",
      "Epoch 441/500, Train Loss: 0.0342, Val Loss: 0.0064, Test Loss: 0.0083\n",
      "Epoch 442/500, Train Loss: 0.0336, Val Loss: 0.0066, Test Loss: 0.0084\n",
      "Epoch 443/500, Train Loss: 0.0369, Val Loss: 0.0064, Test Loss: 0.0084\n",
      "Epoch 444/500, Train Loss: 0.0338, Val Loss: 0.0062, Test Loss: 0.0083\n",
      "Epoch 445/500, Train Loss: 0.0336, Val Loss: 0.0061, Test Loss: 0.0084\n",
      "Epoch 446/500, Train Loss: 0.0342, Val Loss: 0.0061, Test Loss: 0.0084\n",
      "Epoch 447/500, Train Loss: 0.0356, Val Loss: 0.0062, Test Loss: 0.0084\n",
      "Epoch 448/500, Train Loss: 0.0329, Val Loss: 0.0064, Test Loss: 0.0084\n",
      "Epoch 449/500, Train Loss: 0.0355, Val Loss: 0.0066, Test Loss: 0.0085\n",
      "Epoch 450/500, Train Loss: 0.0347, Val Loss: 0.0066, Test Loss: 0.0086\n",
      "Epoch 451/500, Train Loss: 0.0358, Val Loss: 0.0067, Test Loss: 0.0087\n",
      "Epoch 452/500, Train Loss: 0.0323, Val Loss: 0.0066, Test Loss: 0.0087\n",
      "Epoch 453/500, Train Loss: 0.0341, Val Loss: 0.0065, Test Loss: 0.0087\n",
      "Epoch 454/500, Train Loss: 0.0329, Val Loss: 0.0064, Test Loss: 0.0087\n",
      "Epoch 455/500, Train Loss: 0.0340, Val Loss: 0.0064, Test Loss: 0.0088\n",
      "Epoch 456/500, Train Loss: 0.0355, Val Loss: 0.0064, Test Loss: 0.0088\n",
      "Epoch 457/500, Train Loss: 0.0357, Val Loss: 0.0064, Test Loss: 0.0089\n",
      "Epoch 458/500, Train Loss: 0.0344, Val Loss: 0.0065, Test Loss: 0.0089\n",
      "Epoch 459/500, Train Loss: 0.0344, Val Loss: 0.0066, Test Loss: 0.0090\n",
      "Epoch 460/500, Train Loss: 0.0368, Val Loss: 0.0066, Test Loss: 0.0091\n",
      "Epoch 461/500, Train Loss: 0.0339, Val Loss: 0.0066, Test Loss: 0.0091\n",
      "Epoch 462/500, Train Loss: 0.0341, Val Loss: 0.0066, Test Loss: 0.0092\n",
      "Epoch 463/500, Train Loss: 0.0320, Val Loss: 0.0067, Test Loss: 0.0094\n",
      "Epoch 464/500, Train Loss: 0.0366, Val Loss: 0.0067, Test Loss: 0.0093\n",
      "Epoch 465/500, Train Loss: 0.0341, Val Loss: 0.0068, Test Loss: 0.0093\n",
      "Epoch 466/500, Train Loss: 0.0341, Val Loss: 0.0069, Test Loss: 0.0093\n",
      "Epoch 467/500, Train Loss: 0.0357, Val Loss: 0.0070, Test Loss: 0.0093\n",
      "Epoch 468/500, Train Loss: 0.0331, Val Loss: 0.0072, Test Loss: 0.0094\n",
      "Epoch 469/500, Train Loss: 0.0312, Val Loss: 0.0071, Test Loss: 0.0093\n",
      "Epoch 470/500, Train Loss: 0.0351, Val Loss: 0.0068, Test Loss: 0.0091\n",
      "Epoch 471/500, Train Loss: 0.0350, Val Loss: 0.0066, Test Loss: 0.0091\n",
      "Epoch 472/500, Train Loss: 0.0325, Val Loss: 0.0066, Test Loss: 0.0091\n",
      "Epoch 473/500, Train Loss: 0.0325, Val Loss: 0.0065, Test Loss: 0.0090\n",
      "Epoch 474/500, Train Loss: 0.0342, Val Loss: 0.0065, Test Loss: 0.0089\n",
      "Epoch 475/500, Train Loss: 0.0325, Val Loss: 0.0065, Test Loss: 0.0088\n",
      "Epoch 476/500, Train Loss: 0.0335, Val Loss: 0.0064, Test Loss: 0.0088\n",
      "Epoch 477/500, Train Loss: 0.0338, Val Loss: 0.0064, Test Loss: 0.0087\n",
      "Epoch 478/500, Train Loss: 0.0306, Val Loss: 0.0064, Test Loss: 0.0087\n",
      "Epoch 479/500, Train Loss: 0.0288, Val Loss: 0.0064, Test Loss: 0.0086\n",
      "Epoch 480/500, Train Loss: 0.0335, Val Loss: 0.0065, Test Loss: 0.0086\n",
      "Epoch 481/500, Train Loss: 0.0314, Val Loss: 0.0065, Test Loss: 0.0086\n",
      "Epoch 482/500, Train Loss: 0.0333, Val Loss: 0.0064, Test Loss: 0.0086\n",
      "Epoch 483/500, Train Loss: 0.0311, Val Loss: 0.0064, Test Loss: 0.0086\n",
      "Epoch 484/500, Train Loss: 0.0324, Val Loss: 0.0063, Test Loss: 0.0086\n",
      "Epoch 485/500, Train Loss: 0.0323, Val Loss: 0.0063, Test Loss: 0.0086\n",
      "Epoch 486/500, Train Loss: 0.0337, Val Loss: 0.0062, Test Loss: 0.0086\n",
      "Epoch 487/500, Train Loss: 0.0307, Val Loss: 0.0062, Test Loss: 0.0086\n",
      "Epoch 488/500, Train Loss: 0.0319, Val Loss: 0.0062, Test Loss: 0.0086\n",
      "Epoch 489/500, Train Loss: 0.0329, Val Loss: 0.0062, Test Loss: 0.0084\n",
      "Epoch 490/500, Train Loss: 0.0321, Val Loss: 0.0063, Test Loss: 0.0084\n",
      "Epoch 491/500, Train Loss: 0.0286, Val Loss: 0.0064, Test Loss: 0.0084\n",
      "Epoch 492/500, Train Loss: 0.0308, Val Loss: 0.0063, Test Loss: 0.0084\n",
      "Epoch 493/500, Train Loss: 0.0318, Val Loss: 0.0063, Test Loss: 0.0084\n",
      "Epoch 494/500, Train Loss: 0.0326, Val Loss: 0.0063, Test Loss: 0.0084\n",
      "Epoch 495/500, Train Loss: 0.0309, Val Loss: 0.0063, Test Loss: 0.0083\n",
      "Epoch 496/500, Train Loss: 0.0312, Val Loss: 0.0063, Test Loss: 0.0084\n",
      "Epoch 497/500, Train Loss: 0.0316, Val Loss: 0.0063, Test Loss: 0.0084\n",
      "Epoch 498/500, Train Loss: 0.0295, Val Loss: 0.0063, Test Loss: 0.0084\n",
      "Epoch 499/500, Train Loss: 0.0303, Val Loss: 0.0062, Test Loss: 0.0084\n",
      "Epoch 500/500, Train Loss: 0.0293, Val Loss: 0.0062, Test Loss: 0.0084\n"
     ]
    }
   ],
   "source": [
    "def train(model, graph, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out, log_probs = model(batch.x.float(), batch.edge_index)\n",
    "        if task == \"reg\":\n",
    "            y_pred = out\n",
    "        else: \n",
    "            y_pred = log_probs\n",
    "        y_true = batch.y.view(-1, 1)\n",
    "        loss = criterion(y_pred, y_true) #only compute the loss on the train nodes.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def test(model, graph, criterion, loader): #mask parameter added.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            out, log_probs = model(batch.x.float(), batch.edge_index)\n",
    "            if task == \"reg\":\n",
    "                y_pred = out\n",
    "            else: \n",
    "                y_pred = log_probs\n",
    "            y_true = batch.y.view(-1, 1)\n",
    "            loss = criterion(y_pred, y_true) \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    return total_loss / num_batches\n",
    "\n",
    "# Training loop\n",
    "train_losses = []  # List to store losses\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "epochs_list = [] # List to store epoch numbers.\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, graph, optimizer, criterion)\n",
    "    val_loss = test(model, graph, criterion, val_loader) #pass the validation mask\n",
    "    test_loss = test(model, graph, criterion, test_loader) #pass the test mask\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    epochs_list.append(epoch + 1)\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the loss\n",
    "plt.plot(epochs_list, train_losses, label = \"Train Loss\")\n",
    "plt.plot(epochs_list, val_losses, label = \"Validation Loss\")\n",
    "plt.plot(epochs_list, test_losses, label = \"Test Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008391563 0.763477451733516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        out, log_probs = model(batch.x.float(), batch.edge_index)\n",
    "        ypred_gnn = out\n",
    "        y_true = batch.y.view(-1, 1)\n",
    "        val_mse = mean_squared_error(y_true, ypred_gnn)\n",
    "        val_r2 = r2_score(y_true, ypred_gnn)\n",
    "print(val_mse, val_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ypred_gnn < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation MSE: 0.0005\n",
      "Random Forest Validation R2: 0.9828\n",
      "Random Forest Test MSE: 0.0005\n",
      "Random Forest Test R2: 0.9855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def extract_node_features_and_targets(loader):\n",
    "    \"\"\"\n",
    "    Extracts node features and regression targets from a PyTorch Geometric DataLoader.\n",
    "\n",
    "    Args:\n",
    "        loader: PyTorch Geometric DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        features (numpy.ndarray): Node features.\n",
    "        targets (numpy.ndarray): Regression targets.\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_targets = []\n",
    "    for batch in loader:\n",
    "        all_features.append(batch.x.numpy())\n",
    "        all_targets.append(batch.y.numpy())\n",
    "    features = np.concatenate(all_features, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    return features, targets.reshape(-1) #Reshape to 1D array\n",
    "\n",
    "def train_and_evaluate_random_forest(train_loader, val_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a Random Forest Regressor using extracted node features.\n",
    "\n",
    "    Args:\n",
    "        train_loader: PyTorch Geometric DataLoader for training.\n",
    "        val_loader: PyTorch Geometric DataLoader for validation.\n",
    "        test_loader: PyTorch Geometric DataLoader for testing.\n",
    "    \"\"\"\n",
    "    # Extract features and targets\n",
    "    train_features, train_targets = extract_node_features_and_targets(train_loader)\n",
    "    val_features, val_targets = extract_node_features_and_targets(val_loader)\n",
    "    test_features, test_targets = extract_node_features_and_targets(test_loader)\n",
    "\n",
    "    # Initialize and train the Random Forest Regressor\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=91218)  # Adjust parameters as needed\n",
    "    rf.fit(train_features, train_targets)\n",
    "\n",
    "    # Evaluate on validation set (for hyperparameter tuning if needed)\n",
    "    val_predictions = rf.predict(val_features)\n",
    "    val_mse = mean_squared_error(val_targets, val_predictions)\n",
    "    val_r2 = r2_score(val_targets, val_predictions)\n",
    "    print(f\"Random Forest Validation MSE: {val_mse:.4f}\")\n",
    "    print(f\"Random Forest Validation R2: {val_r2:.4f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_predictions = rf.predict(test_features)\n",
    "    test_mse = mean_squared_error(test_targets, test_predictions)\n",
    "    test_r2 = r2_score(test_targets, test_predictions)\n",
    "    print(f\"Random Forest Test MSE: {test_mse:.4f}\")\n",
    "    print(f\"Random Forest Test R2: {test_r2:.4f}\")\n",
    "\n",
    "    return test_mse, test_r2, test_predictions\n",
    "\n",
    "# Example Usage (assuming you have train_loader, val_loader, test_loader)\n",
    "# train_loader, val_loader, test_loader are pytorch geometric dataloaders\n",
    "# Example:\n",
    "train_mse, train_r2, ypred = train_and_evaluate_random_forest(train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(ypred < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == \"reg\":\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "psAwbtMpPgqP"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=2 must be between 1 and min(n_samples, n_features)=1 with svd_solver='randomized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m h, _ \u001b[38;5;241m=\u001b[39m untrained(graph\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mfloat(), graph\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train TSNE\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tsne \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m----> 6\u001b[0m \u001b[43m         \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpca\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m unique_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(graph\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModerate Poor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-vulnerable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSevere Poor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVulnerable\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1111\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m-> 1111\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:984\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;66;03m# Always output a numpy array, no matter what is configured globally\u001b[39;00m\n\u001b[0;32m    983\u001b[0m pca\u001b[38;5;241m.\u001b[39mset_output(transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 984\u001b[0m X_embedded \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# PCA is rescaled so that PC1 has standard deviation 1e-4 which is\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# the default value for random initialization. See issue #18018.\u001b[39;00m\n\u001b[0;32m    987\u001b[0m X_embedded \u001b[38;5;241m=\u001b[39m X_embedded \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(X_embedded[:, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:460\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    439\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:512\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_full(X, n_components)\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_truncated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_svd_solver\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:585\u001b[0m, in \u001b[0;36mPCA._fit_truncated\u001b[1;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m cannot be a string with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;241m%\u001b[39m (n_components, svd_solver)\n\u001b[0;32m    583\u001b[0m     )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m must be between 1 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    587\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    589\u001b[0m         \u001b[38;5;241m%\u001b[39m (n_components, \u001b[38;5;28mmin\u001b[39m(n_samples, n_features), svd_solver)\n\u001b[0;32m    590\u001b[0m     )\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m n_components \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    593\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m must be strictly less than \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    595\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    596\u001b[0m         \u001b[38;5;241m%\u001b[39m (n_components, \u001b[38;5;28mmin\u001b[39m(n_samples, n_features), svd_solver)\n\u001b[0;32m    597\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=2 must be between 1 and min(n_samples, n_features)=1 with svd_solver='randomized'"
     ]
    }
   ],
   "source": [
    "# Get embeddings\n",
    "h, _ = untrained(graph.x.float(), graph.edge_index)\n",
    "\n",
    "# Train TSNE\n",
    "tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "         init='pca').fit_transform(h.detach())\n",
    "\n",
    "unique_labels = np.unique(graph.y)\n",
    "labels = ['Moderate Poor', 'Non-vulnerable', 'Severe Poor', 'Vulnerable']\n",
    "\n",
    "# Plot TSNE\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis('off')\n",
    "# Scatter plot with different colors for each class\n",
    "for label in unique_labels:\n",
    "    indices = graph.y == label\n",
    "    plt.scatter(tsne[indices, 0], tsne[indices, 1], s=20, label=labels[label])\n",
    "    # Add legend\n",
    "plt.legend(title=\"Class Labels\", loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFcvr6KMPkNV"
   },
   "outputs": [],
   "source": [
    "h, _ = model(graph.x.float(), graph.edge_index)\n",
    "\n",
    "# Train TSNE\n",
    "tsne = TSNE(n_components=2, learning_rate='auto',\n",
    "         init='pca').fit_transform(h.detach())\n",
    "\n",
    "unique_labels = np.unique(graph.y)\n",
    "labels = ['Moderate Poor', 'Non-vulnerable', 'Severe Poor', 'Vulnerable']\n",
    "\n",
    "# Plot TSNE\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis('off')\n",
    "# Scatter plot with different colors for each class\n",
    "for label in unique_labels:\n",
    "    indices = graph.y == label\n",
    "    plt.scatter(tsne[indices, 0], tsne[indices, 1], s=20, label=labels[label])\n",
    "# Add legend\n",
    "plt.legend(title=\"Class Labels\", loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(graph.x.float(), graph.edge_index)\n",
    "i, y_pred = output[1].max(1)\n",
    "y_true = graph.y\n",
    "cm = confusion_matrix(graph.y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(len(labels),len(labels)))\n",
    "cm_prop = cm.astype('float') / cm.sum(axis=1)\n",
    "sns.heatmap(cm_prop, annot = True, xticklabels=labels, yticklabels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(len(labels),len(labels)))\n",
    "sns.heatmap(cm.astype('int'), annot = True, xticklabels=labels, yticklabels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "cWFE3gnUo7nA"
   },
   "source": [
    "class GCN(torch.nn.Module):\n",
    "  \"\"\"Graph Convolutional Network\"\"\"\n",
    "  def __init__(self, dim_in, dim_h, dim_out):\n",
    "    super().__init__()\n",
    "    self.gcn1 = GCNConv(dim_in, dim_h)\n",
    "    self.gcn2 = GCNConv(dim_h, dim_out)\n",
    "    self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                      lr=0.01,\n",
    "                                      0)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h = F.dropout(x, p=0.5)\n",
    "    h = self.gcn1(h, edge_index)\n",
    "    h = torch.relu(h)\n",
    "    h = F.dropout(h, p=0.5)\n",
    "    h = self.gcn2(h, edge_index)\n",
    "    return h, F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "cWFE3gnUo7nA"
   },
   "source": [
    "class GAT(torch.nn.Module):\n",
    "  \"\"\"Graph Attention Network\"\"\"\n",
    "  def __init__(self, dim_in, dim_h, dim_out, heads=8):\n",
    "    super().__init__()\n",
    "    self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)\n",
    "    self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1)\n",
    "    self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                      lr=0.005,\n",
    "                                      weight_decay=5e-4)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h = F.dropout(x, p=0.6, training=self.training)\n",
    "    h = self.gat1(x, edge_index)\n",
    "    h = F.elu(h)\n",
    "    h = F.dropout(h, p=0.6, training=self.training)\n",
    "    h = self.gat2(h, edge_index)\n",
    "    return h, F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "cWFE3gnUo7nA"
   },
   "source": [
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def train(model, data):\n",
    "    \"\"\"Train a GNN model and return the trained model.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = model.optimizer\n",
    "    epochs = 200\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        # Training\n",
    "        optimizer.zero_grad()\n",
    "        _, out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "        val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n",
    "\n",
    "        # Print metrics every 10 epochs\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: '\n",
    "                  f'{acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
    "                  f'Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "    return model\n",
    "\n",
    "def test(model, data):\n",
    "    \"\"\"Evaluate the model on test set and print the accuracy score.\"\"\"\n",
    "    model.eval()\n",
    "    _, out = model(data.x, data.edge_index)\n",
    "    acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
    "    return acc"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
